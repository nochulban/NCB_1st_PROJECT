import pandas as pd
import requests
import pymysql
import connectDatabase
from fpdf import FPDF
import os
from datetime import datetime

api_key = ""   # ğŸ‘‰ OpenAI API í‚¤ ì…ë ¥
os.chdir(os.path.dirname(os.path.abspath(__file__)))

# ------------------ GPTë¡œ ê°œìš”+ìš”ì•½ ìƒì„± ------------------

def get_summary_from_gpt(keyword, data_sample, key):
    url = "https://api.openai.com/v1/chat/completions"
    headers = {
        "Content-Type": "application/json",
        "Authorization": f"Bearer {key}"
    }
    prompt = (
    "ë‹¹ì‹ ì€ í´ë¼ìš°ë“œ ë³´ì•ˆ íƒì§€ ë³´ê³ ì„œë¥¼ ì‘ì„±í•˜ëŠ” AIì…ë‹ˆë‹¤.\n"
        "ì´ ë³´ê³ ì„œëŠ” ê³µê°œëœ ì˜¤ë¸Œì íŠ¸ ìŠ¤í† ë¦¬ì§€(S3/NCP ë“±)ì—ì„œ ìˆ˜ì§‘í•œ ë¬¸ì„œë“¤ì˜ ë©”íƒ€ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìë™ ìƒì„±ë©ë‹ˆë‹¤.\n"
        "ì•„ë˜ ìƒ˜í”Œ ë°ì´í„°ëŠ” URL, í™•ì¥ì, ìˆ˜ì§‘ì¼, ì†Œì† ë²„í‚· ë“±ìœ¼ë¡œ êµ¬ì„±ëœ ë©”íƒ€ë°ì´í„°ì…ë‹ˆë‹¤.\n"
        "\n"
        "ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ë³´ê³ ì„œë¥¼ ì‘ì„±í•˜ì‹­ì‹œì˜¤. í˜•ì‹ì€ ë§ˆí¬ë‹¤ìš´ì´ ì•„ë‹Œ **í…ìŠ¤íŠ¸ ê¸°ë°˜(plain text)**ì…ë‹ˆë‹¤.\n"
        "\n"
        "==============================\n"
        "ê³µê°œ S3 ë²„í‚· ëŒ€ìƒ ìœ ì¶œ ì˜ì‹¬ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ ë³´ê³ ì„œ\n"
        "==============================\n"
        "\n"
        "[ë³´ê³ ì„œ ê°œìš”]\n"
        f"ê²€ìƒ‰ í‚¤ì›Œë“œ '{keyword}'ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìë™í™”ëœ ë³´ì•ˆ ì ê²€ ì‹œìŠ¤í…œì„ í†µí•´ ê³µê°œ ì˜¤ë¸Œì íŠ¸ ìŠ¤í† ë¦¬ì§€ ë‚´ì—ì„œ ìˆ˜ì§‘ëœ ìœ ì¶œ ì˜ì‹¬ íŒŒì¼ë“¤ì„ ë¶„ì„í•˜ì˜€ìŠµë‹ˆë‹¤. ë³¸ ë³´ê³ ì„œëŠ” ìˆ˜ì§‘ ëŒ€ìƒì˜ ê¸°ë³¸ ì •ë³´, ìˆ˜ì§‘ ë°©ì‹, íƒì§€ ê²°ê³¼ë¥¼ í¬í•¨í•©ë‹ˆë‹¤.\n"
        "\n"
        "[íƒì§€ ìš”ì•½]\n"
        "- ì´ íƒì§€ ìˆ˜: (ìë™ ê³„ì‚°)\n"
        "- ê³ ìœ  ë¬¸ì„œ ìˆ˜: (ìë™ ê³„ì‚°)\n"
        "- ê³µê°œ ë²„í‚· ìˆ˜: (ìë™ ê³„ì‚°)\n"
        "\n"
        "[ìœ ì¶œ ë¬¸ì„œ ìƒì„¸ ë¦¬ìŠ¤íŠ¸]\n"
        "ê° í•­ëª©ì€ í•œ ì¤„ì”© êµ¬ë¶„í•˜ì—¬ ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ë‚˜ì—´:\n"
        "  URL: {bucket_url}\n"
        "  íŒŒì¼ìˆ˜: {file_count}\n"
        "  ìˆ˜ì§‘ì¼: {dt}\n"
        "\n"
        "[ê²°ë¡  ë° ê¶Œê³ ì‚¬í•­]\n"
        "íƒì§€ëœ ë¬¸ì„œë“¤ì€ ì™¸ë¶€ì— ë¯¼ê° ì •ë³´ê°€ ë…¸ì¶œë˜ì—ˆì„ ê°€ëŠ¥ì„±ì´ ìˆìœ¼ë©°, ì¦‰ê°ì ì¸ ì ‘ê·¼ ì œì–´ ë° ë‚´ë¶€ ë³´ì•ˆ ì ê²€ì´ ìš”êµ¬ë©ë‹ˆë‹¤. ë³´ì•ˆ ë‹´ë‹¹ìì—ê²Œ ë‹¤ìŒê³¼ ê°™ì€ ì¡°ì¹˜ë¥¼ ê¶Œê³ í•©ë‹ˆë‹¤:\n"
        "- ê³µê°œ ë²„í‚· ì„¤ì • ì—¬ë¶€ ê²€í†  ë° ë¹„ê³µê°œ ì „í™˜\n"
        "- ë¬¸ì„œ ë‚´ í¬í•¨ëœ ì •ë³´ì˜ ë¯¼ê°ë„ í™•ì¸\n"
        "- ìë™ ì ê²€ ì‹œìŠ¤í…œì˜ ì •ê¸° ì‹¤í–‰ ë„ì…\n"
        "\n"
        #"â€» ì´ ë³´ê³ ì„œëŠ” Python ê¸°ë°˜ ìë™í™” ë„êµ¬ì— ì˜í•´ PDFë¡œ ì¶œë ¥ë©ë‹ˆë‹¤. í•­ëª© ê°„ ê°„ê²© ë° ì¤„ë°”ê¿ˆì„ ê³ ë ¤í•´ ì¶œë ¥ë˜ë„ë¡ êµ¬ì„±í•´ ì£¼ì„¸ìš”.\n"
        "\n"
        f"ìƒ˜í”Œ ë°ì´í„°:\n{data_sample}"
    )
    data = {
        "model": "gpt-3.5-turbo",
        "messages": [{"role": "user", "content": prompt}],
        "temperature": 0,
        "max_tokens": 2048
    }

    response = requests.post(url, headers=headers, json=data, timeout=200)
    response.raise_for_status()
    result = response.json()
    return result['choices'][0]['message']['content']

# ------------------ MySQL ì—°ê²° ë° ë°ì´í„° ê°€ì ¸ì˜¤ê¸° ------------------

def load_mysql_table_to_dataframe():

    rows = connectDatabase.setDataFrame()
    df = pd.DataFrame(rows)
    return df

# ------------------ PDF ë³´ê³ ì„œ ì €ì¥ ------------------

def save_report_to_pdf(pdf_path, summary_text, df):
    pdf = FPDF(orientation='P', unit='mm', format='A4')
    pdf.add_page()
    pdf.add_font("NanumGothic", '', "NanumGothic.ttf", uni=True)
    pdf.set_font("NanumGothic", size=14)

    # ê°œìš” ë° íƒì§€ ìš”ì•½
    pdf.multi_cell(0, 10, "1. ë³´ê³ ì„œ ê°œìš” ë° íƒì§€ ìš”ì•½")
    pdf.set_font("NanumGothic", size=12)
    for line in summary_text.split('\n'):
        pdf.multi_cell(0, 10, line)

    # ìœ ì¶œ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
    # pdf.ln(5)
    # pdf.set_font("NanumGothic", size=14)
    # pdf.multi_cell(0, 10, "2. ìœ ì¶œ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸")
    # pdf.set_font("NanumGothic", size=12)

    if 'file_name' in df.columns and 'bucket_url' in df.columns:
        bucket_count = df['bucket_url'].nunique()
        file_count = df[''].nunique()
        pdf.multi_cell(0, 10, f"íƒì§€ëœ ì „ì²´ ë²„í‚· ìˆ˜: {bucket_count}ê°œ")
        pdf.multi_cell(0, 10, f"íƒì§€ëœ ì „ì²´ íŒŒì¼ ìˆ˜: {file_count}ê°œ")
        pdf.ln(5)

        grouped = df.groupby('bucket_url')
        for bucket, group in grouped:
            pdf.ln(4)
            pdf.set_font("NanumGothic", size=12)
            pdf.multi_cell(0, 8, f"[ë²„í‚· ì£¼ì†Œ] {bucket}")
            for idx, row in group.iterrows():
                filename = str(row.get('file_name', 'íŒŒì¼ëª… ì—†ìŒ'))
                #extension = str(row.get('extension', 'í™•ì¥ì ì—†ìŒ'))
                file_count = str(row.get('file_count', 'í™•ì¥ì ì—†ìŒ'))
                #file_size = str(row.get('file_size', 'í¬ê¸° ì •ë³´ ì—†ìŒ'))
                pdf.multi_cell(0, 8, f"- {filename}, {file_count} ")
    else:
        print("íŒŒì¼ëª… ë˜ëŠ” ë²„í‚· ì£¼ì†Œ ì •ë³´ ì—†ìŒ")

    pdf.output(pdf_path, "F")

# ------------------ ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ------------------

def run_pipeline(keyword):
    df = load_mysql_table_to_dataframe()

    sample = df.head(5).to_string(index=False)
    print(df)

    summary_text = get_summary_from_gpt(keyword, df, api_key)

    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    pdf_path = f"output/report_{timestamp}.pdf"

    save_report_to_pdf(pdf_path, summary_text, df)
    print(f"âœ… PDF ë³´ê³ ì„œê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤: {pdf_path}")

    #connectDatabase.truncateBucketTable()
    #connectDatabase.truncateDocumentsTable()

# ------------------ ë©”ì¸ ì‹¤í–‰ ------------------

#if __name__ == "__main__":
    
    ## ì €ì¥í•  íŒŒì¼ ê²½ë¡œ ì„¤ì •
    ##ppt_save_path = "bucket_report.pptx"
    ##docx_save_path = "bucket_report.docx"    

    # í…ŒìŠ¤íŠ¸ì‹œ ì£¼ì„ í•´ì œ 
    # keyword = input()
    # run_pipeline(keyword)
